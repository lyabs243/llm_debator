{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2275c7",
   "metadata": {},
   "source": [
    "## Conversational Requests\n",
    "Interaction with contents and systems prompts using one model or combination of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0018ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db24887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init models names\n",
    "GPT_4_1_MINI = \"gpt-4.1-mini\"\n",
    "GPT_4_1 = \"gpt-4.1\"\n",
    "GPT_5_MINI = \"gpt-5-mini\"\n",
    "GPT_5 = \"gpt-5\"\n",
    "\n",
    "GEMINI_2_5_FLASH = \"gemini-2.5-flash\"\n",
    "GEMINI_2_5_PRO = \"gemini-2.5-pro\"\n",
    "\n",
    "GROQ_LLAMA_3_1 = \"llama-3.1-8b-instant\"\n",
    "GROQ_LLAMA_3_3 = \"`llama-3.3-70b-versatile\"\n",
    "GROQ_GPT_OSS_20 = \"openai/gpt-oss-20b\"\n",
    "GROQ_GPT_OSS_120 = \"openai/gpt-oss-120b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d89e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init api keys\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ca8626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init clients\n",
    "openai_client = OpenAI()\n",
    "\n",
    "gemini_client = OpenAI(\n",
    "    api_key=gemini_api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "groq_client = OpenAI(\n",
    "    api_key=groq_api_key,\n",
    "    base_url=\"https://api.groq.com/openai/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aea0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_message(client: OpenAI, model: str, message: str, messages: list[dict], max_tokens: int = 1024) -> tuple[str|None, list[dict]]:\n",
    "    \"\"\"\n",
    "    Send a message to the specified model using the provided OpenAI client.\n",
    "\n",
    "    Args:\n",
    "        client (OpenAI): The OpenAI client to use for the request.\n",
    "        model (str): The model name to use for the request.\n",
    "        message (str): The user message to send.\n",
    "        messages (list[dict]): The conversation history.\n",
    "        max_tokens (int): The maximum number of tokens to generate in the response.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str|None, list[dict]]: The model's response and the updated conversation history.\n",
    "    \"\"\"\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_completion_tokens=max_tokens\n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "        messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending message: {e}\")\n",
    "    return answer, messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d7a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_messages = []\n",
    "gemini_messages = []\n",
    "groq_messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a3962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, openai_messages = send_message(\n",
    "    client=openai_client,\n",
    "    model=GPT_4_1,\n",
    "    message=\"Hello, my name is Loic. Can you help me?\",\n",
    "    messages=openai_messages,\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c14dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, openai_messages = send_message(\n",
    "    client=openai_client,\n",
    "    model=GPT_4_1,\n",
    "    message=\"Do you remember my name?\",\n",
    "    messages=openai_messages\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e615623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04362a98",
   "metadata": {},
   "source": [
    "### Mutliple Turns Conversation\n",
    "Let's have fun with multiple turns conversation combining different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b23bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_prompt(favorite_player: str, opponent_player: str) -> str:\n",
    "    return f\"\"\"You are a fan of {favorite_player}, you are debating in favor of him against {opponent_player}'s fan. Be happy and enthusiastic.\n",
    "    \n",
    "    Be tough in debates by throwing out punchlines and using emojis, and reply with short messages.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_messages = [\n",
    "    {\"role\": \"system\", \"content\": get_system_prompt(\"Lionel Messi\", \"Cristiano Ronaldo\")},\n",
    "]\n",
    "\n",
    "gemini_messages = [\n",
    "    {\"role\": \"system\", \"content\": get_system_prompt(\"Cristiano Ronaldo\", \"Lionel Messi\")},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ef58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_turns = 5\n",
    "\n",
    "openai_message = \"\"\n",
    "gemini_message = \"Hey how are you?\"\n",
    "\n",
    "print(f\"Gemini (Cristiano Ronaldo): {gemini_message}\")\n",
    "\n",
    "for turn in range(max_turns):\n",
    "    openai_message, openai_messages = send_message(\n",
    "        client=openai_client,\n",
    "        model=GPT_5_MINI,\n",
    "        message=gemini_message,\n",
    "        messages=openai_messages,\n",
    "    )\n",
    "    display(Markdown(f\"**OpenAI (Lionel Messi):** {openai_message}\"))\n",
    "\n",
    "    gemini_message, gemini_messages = send_message(\n",
    "        client=gemini_client,\n",
    "        model=GEMINI_2_5_FLASH,\n",
    "        message=openai_message,\n",
    "        messages=gemini_messages,\n",
    "    )\n",
    "    display(Markdown(f\"**Gemini (Cristiano Ronaldo):** {gemini_message}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dce3d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebatorItem:\n",
    "    def __init__(self, client: OpenAI, model: str, favorite: str):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.favorite = favorite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debate_models(debator1: DebatorItem, debator2: DebatorItem, max_turns: int, initial_message: str):\n",
    "    \"\"\"\n",
    "    Conduct a debate between two models with system prompts based on their favorite players.\n",
    "\n",
    "    Args:\n",
    "        debator1 (DebatorItem): The first debator item containing client, model, and favorite player.\n",
    "        debator2 (DebatorItem): The second debator item containing client, model,\n",
    "        max_turns: Maximum number of debate turns.\n",
    "        initial_message: The initial message from the second model's fan.\n",
    "    \"\"\"\n",
    "    system_prompt1 = get_system_prompt(debator1.favorite, debator2.favorite)\n",
    "    system_prompt2 = get_system_prompt(debator2.favorite, debator1.favorite)\n",
    "\n",
    "    messages1 = [{\"role\": \"system\", \"content\": system_prompt1}]\n",
    "    messages2 = [{\"role\": \"system\", \"content\": system_prompt2}]\n",
    "    \n",
    "    message2 = initial_message\n",
    "    display(Markdown(f\"**{debator2.favorite} fan ({debator2.model}):** {initial_message}\"))\n",
    "    \n",
    "    for turn in range(max_turns):\n",
    "        message1, messages1 = send_message(debator1.client, debator1.model, message2, messages1)\n",
    "        display(Markdown(f\"**{debator1.favorite} fan ({debator1.model}):** {message1}\"))\n",
    "\n",
    "        message2, messages2 = send_message(debator2.client, debator2.model, message1, messages2)\n",
    "        display(Markdown(f\"**{debator2.favorite} fan ({debator2.model}):** {message2}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb94a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "debate_models(\n",
    "    debator1=DebatorItem(\n",
    "        client=groq_client,\n",
    "        model=GROQ_LLAMA_3_1,\n",
    "        favorite=\"Lionel Messi\"\n",
    "    ),\n",
    "    debator2=DebatorItem(\n",
    "        client=gemini_client,\n",
    "        model=GEMINI_2_5_FLASH,\n",
    "        favorite=\"Cristiano Ronaldo\"\n",
    "    ),\n",
    "    max_turns=3,\n",
    "    initial_message=\"Hey how are you?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca10f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun debat between Gemini and Groq GPT-OSS 20B about tabs vs spaces ðŸ˜‚\n",
    "debate_models(\n",
    "    debator1=DebatorItem(\n",
    "        client=groq_client,\n",
    "        model=GROQ_GPT_OSS_20,\n",
    "        favorite=\"Tabs\"\n",
    "    ),\n",
    "    debator2=DebatorItem(\n",
    "        client=gemini_client,\n",
    "        model=GEMINI_2_5_FLASH,\n",
    "        favorite=\"Spaces\"\n",
    "    ),\n",
    "    max_turns=4,\n",
    "    initial_message=\"Hey how are you?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b139765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political debat between Gemini and Groq Llama 3.1 about Puttinng President picture in every classroom vs not putting it ðŸ˜‚\n",
    "debate_models(\n",
    "    debator1=DebatorItem(\n",
    "        client=groq_client,\n",
    "        model=GROQ_LLAMA_3_1,\n",
    "        favorite=\"Putting President picture every classroom\"\n",
    "    ),\n",
    "    debator2=DebatorItem(\n",
    "        client=gemini_client,\n",
    "        model=GEMINI_2_5_FLASH,\n",
    "        favorite=\"Not putting President picture every classroom\"\n",
    "    ),\n",
    "    max_turns=4,\n",
    "    initial_message=\"Salut comment Ã§a va ?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7039e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elon Musk vs Mark Zuckerberg debate about who is the best CEO ðŸ˜‚\n",
    "debate_models(\n",
    "    debator1=DebatorItem(\n",
    "        client=groq_client,\n",
    "        model=GROQ_GPT_OSS_20,\n",
    "        favorite=\"Elon Musk\"\n",
    "    ),\n",
    "    debator2=DebatorItem(\n",
    "        client=gemini_client,\n",
    "        model=GEMINI_2_5_FLASH,\n",
    "        favorite=\"Mark Zuckerberg\"\n",
    "    ),\n",
    "    max_turns=4,\n",
    "    initial_message=\"Hey how are you?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e22bab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_debator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
